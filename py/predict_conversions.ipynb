{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Conversions from Quotes\n",
    "\n",
    "**Goal**: Build a model to predict whether a quote will convert to a purchase.\n",
    "\n",
    "**Data Sources**\n",
    "* MySQL records of quote details and outcome\n",
    "* Paypal records of flooring samples purchases\n",
    "* Mailchimp subscriber list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bd_mailchimp\n",
    "import bd_paypal\n",
    "import bd_mysql\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load mailchimp data\n",
    "csv_path = '/Users/lindsay/Documents/Data Science/BrazilianDirect/csv/mailchimp/members_export_21_march_2016.csv'\n",
    "mail, first_email = bd_mailchimp.process_mailchimp(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load paypal data\n",
    "csv_dir = '/Users/lindsay/Documents/Data Science/BrazilianDirect/csv/paypal/'\n",
    "paypal, first_sample = bd_paypal.process_paypal(csv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load MySQL data\n",
    "config_path = '/Users/lindsay/Documents/Data Science/BrazilianDirect/cfg/mysql.cfg'\n",
    "con = bd_mysql.connect_bd_mysql(config_path)\n",
    "df = bd_mysql.download_quote_data(con)\n",
    "df = bd_mysql.pre_process_mysql(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge mysql & mail chimp\n",
    "df_all = pd.merge(df, mail, how='left', on='email')\n",
    "\n",
    "# add paypal\n",
    "df_all = pd.merge(df_all, paypal, how='left', on='email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace nan with 0\n",
    "df_all['mail_chimp'] = df_all['mail_chimp'].fillna(value=0)\n",
    "df_all['samples'] = df_all['samples'].fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out quotes before mail chimp & samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20821, 26)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliest_date = pd.datetime.date(max(first_email, first_sample))\n",
    "df_all = df_all.loc[df_all['date_created'] >= earliest_date, :]\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop columns that won't be used for predictions\n",
    "\n",
    "* `quote_id`: unique identifier\n",
    "* `email`: nearly unique identifier (not many repeat customers)\n",
    "* `date_created`: too fine grained to use\n",
    "* `days_until_needed`: transformed into a binned variable\n",
    "* `ship_state`: transformed into a grouped variable for regions\n",
    "* `install_subfloor`: too many missing values\n",
    "* `sq_ft`: transformed into a binned variable\n",
    "* `milling`: milling is perfectly correlated with `finish` (unfinished = square edge, prefinished = micro bevel)\n",
    "* `year`: interested only in monthly seasonality\n",
    "* `state_division`: will use the regional divisions, which have fewer categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_cols = ['quote_id',\n",
    "             'email',\n",
    "             'date_created',\n",
    "             'days_until_needed',\n",
    "             'ship_state',\n",
    "             'install_subfloor',\n",
    "             'sq_ft',\n",
    "             'milling',\n",
    "             'year',\n",
    "             'state_division']\n",
    "df_all = df_all.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract variables from data fame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'phone_provided', u'employee_id', u'cust_price', u'retail_price',\n",
       "       u'common_name', u'finish', u'grade', u'width', u'construction',\n",
       "       u'days_until_needed_bin', u'sq_ft_bin', u'converted', u'month',\n",
       "       u'state_region', u'mail_chimp', u'samples'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# column types\n",
    "dependent_column = ['converted']\n",
    "cat_columns = ['phone_provided',\n",
    "               'employee_id',\n",
    "               'common_name',\n",
    "               'finish',\n",
    "               'grade', \n",
    "               'width', \n",
    "               'construction',\n",
    "               'days_until_needed_bin',\n",
    "               'sq_ft_bin',\n",
    "               'month', \n",
    "               'state_region',\n",
    "               'mail_chimp',\n",
    "               'samples']\n",
    "meas_columns = ['cust_price',\n",
    "                'retail_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract variables\n",
    "y = df_all.as_matrix(columns=dependent_column).ravel()\n",
    "x_cat = df_all.as_matrix(columns=cat_columns)\n",
    "x_meas = df_all.as_matrix(columns=meas_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform categorical variables to 0-indexed integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/numpy/lib/arraysetops.py:200: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  flag = np.concatenate(([True], aux[1:] != aux[:-1]))\n",
      "//anaconda/lib/python2.7/site-packages/numpy/lib/arraysetops.py:259: FutureWarning: numpy equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  return aux[:-1][aux[1:] == aux[:-1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labels = {}\n",
    "for i, col in enumerate(cat_columns):\n",
    "    \n",
    "    # Extract the column values and convert to 0-indexed integers\n",
    "    x = x_cat[:, i]\n",
    "    le = LabelEncoder().fit(x)\n",
    "    x_cat[:, i] = le.transform(x)\n",
    "    \n",
    "    # Store label data\n",
    "    labels[col] = le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values\n",
    "We'll impute with the median value for the numerical columns that are missing data (`cust_price`) and impute with the mode value for missing categorical data (`days_until_needed_bin` and `sq_ft_bin`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phone_provided :  0\n",
      "employee_id :  0\n",
      "cust_price :  842\n",
      "retail_price :  688\n",
      "common_name :  0\n",
      "finish :  0\n",
      "grade :  0\n",
      "width :  0\n",
      "construction :  0\n",
      "days_until_needed_bin :  10174\n",
      "sq_ft_bin :  3\n",
      "converted :  0\n",
      "month :  0\n",
      "state_region :  0\n",
      "mail_chimp :  0\n",
      "samples :  0\n"
     ]
    }
   ],
   "source": [
    "# Examine number of missing values in each column\n",
    "for col in df_all.columns:\n",
    "    print col, ': ', df_all[col].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Impute measured variables\n",
    "imp_meas = Imputer(missing_values=np.nan, strategy='median', axis=0)\n",
    "\n",
    "# Fit imputer\n",
    "imp_meas.fit(x_meas)\n",
    "\n",
    "# Impute values\n",
    "x_meas_imp = imp_meas.transform(x_meas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Impute categorical variables\n",
    "imp_cat = Imputer(missing_values=np.nan, strategy='most_frequent', axis=0)\n",
    "\n",
    "# Fit imputer\n",
    "imp_cat.fit(x_cat)\n",
    "\n",
    "# Impute values\n",
    "x_cat_imp = imp_cat.transform(x_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform one-hot encoding on categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use one-hot encoding to transform categorical variable into\n",
    "# multiple binary variables\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(x_cat_imp)\n",
    "\n",
    "# Apply one-hot encoding\n",
    "x_cat_imp = enc.transform(x_cat_imp).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a list so we know what the one-hot encoded columns refer to\n",
    "one_hot_columns = []\n",
    "for col in cat_columns:\n",
    "    features = list(labels[col].classes_)\n",
    "    for f in features:\n",
    "        one_hot_columns.append(col + '_' + str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "x_meas_imp = scale(x_meas_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate categorical and continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.concatenate((x_cat_imp, x_meas_imp), axis=1)\n",
    "x_names = one_hot_columns + meas_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],\n",
       "           class_weight='balanced', cv=10, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1.0, max_iter=100, multi_class='ovr',\n",
       "           n_jobs=1, penalty='l2', random_state=None, refit=True,\n",
       "           scoring='average_precision', solver='lbfgs', tol=0.0001,\n",
       "           verbose=0)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "lr = LogisticRegressionCV(class_weight='balanced', \n",
    "                          scoring='average_precision',\n",
    "                          Cs=c,\n",
    "                          cv=10)\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = lr.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.28596265,  0.28595905,  0.07460258,  0.02144461, -0.1623039 ,\n",
       "         0.06625311,  0.0432307 , -0.06283856,  0.05041167, -0.05847393,\n",
       "        -0.02640949,  0.01249368,  0.04512746,  0.08056245, -0.0841076 ,\n",
       "        -0.02109784, -0.04115682,  0.06225105, -0.05717751,  0.01532117,\n",
       "        -0.00113039,  0.04298314, -0.00135473,  0.05884321,  0.03275145,\n",
       "        -0.06135874,  0.04821791, -0.07710269, -0.03050627, -0.00831092,\n",
       "         0.03881359,  0.1566717 , -0.08094491, -0.03949624,  0.05261838,\n",
       "        -0.08885253,  0.00255546,  0.02798396,  0.03206615, -0.02804292,\n",
       "         0.00036489, -0.03378837, -0.00114277, -0.02124431, -0.05217767,\n",
       "         0.00084033, -0.00607577,  0.01945987, -0.00598102,  0.0135713 ,\n",
       "        -0.03030499,  0.01584463,  0.01238716,  0.03247149,  0.02120538,\n",
       "        -0.14561727,  0.00150676,  0.18503027,  0.02491219, -0.06583555,\n",
       "        -0.12374915,  0.12374555, -0.32732683,  0.32732323, -0.08818736,\n",
       "         0.12511332]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_order = list(np.argsort(np.abs(lr.coef_[0]))[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg  -  samples_0.0\n",
      "pos  -  samples_1.0\n",
      "neg  -  phone_provided_0\n",
      "pos  -  phone_provided_1\n",
      "pos  -  state_region_northeast\n",
      "neg  -  employee_id_OTHER\n",
      "pos  -  days_until_needed_bin_nan\n",
      "neg  -  state_region_Other\n",
      "pos  -  retail_price\n",
      "neg  -  mail_chimp_0.0\n",
      "pos  -  mail_chimp_1.0\n",
      "neg  -  days_until_needed_bin_91+\n",
      "neg  -  cust_price\n",
      "neg  -  common_name_Tigerwood\n",
      "neg  -  days_until_needed_bin_0-30\n",
      "pos  -  common_name_Tiete Rosewood\n",
      "neg  -  width_Other\n",
      "pos  -  employee_id_DC\n",
      "pos  -  employee_id_SO\n",
      "neg  -  state_region_west\n",
      "neg  -  common_name_Brazilian Cherry\n",
      "pos  -  finish_Unfinished\n",
      "neg  -  width_4 3/4\n",
      "pos  -  width_3 1/4\n",
      "neg  -  common_name_Brazilian Walnut\n",
      "neg  -  grade_Clear\n",
      "pos  -  days_until_needed_bin_61-90\n",
      "neg  -  month_2.0\n",
      "pos  -  common_name_Brazilian Teak\n",
      "pos  -  width_5\n",
      "pos  -  common_name_Santos Mahogany\n",
      "pos  -  common_name_Amendoim\n",
      "pos  -  grade_Select & Better\n",
      "neg  -  finish_Prefinished\n",
      "neg  -  days_until_needed_bin_31-60\n",
      "pos  -  construction_Solid\n",
      "neg  -  sq_ft_bin_5001+\n",
      "pos  -  width_4\n",
      "pos  -  month_11.0\n",
      "pos  -  sq_ft_bin_1001-1500\n",
      "neg  -  construction_Engineered\n",
      "neg  -  month_8.0\n",
      "neg  -  sq_ft_bin_1501-2000\n",
      "pos  -  sq_ft_bin_0-500\n",
      "neg  -  common_name_Other\n",
      "pos  -  state_region_south\n",
      "pos  -  employee_id_NP\n",
      "neg  -  month_1.0\n",
      "pos  -  month_12.0\n",
      "neg  -  finish_Missing\n",
      "pos  -  month_5.0\n",
      "pos  -  month_9.0\n",
      "pos  -  grade_Missing\n",
      "pos  -  month_7.0\n",
      "pos  -  common_name_Patagonian Rosewood\n",
      "pos  -  month_10.0\n",
      "neg  -  construction_Missing\n",
      "neg  -  month_4.0\n",
      "neg  -  month_6.0\n",
      "pos  -  sq_ft_bin_nan\n",
      "pos  -  state_region_midwest\n",
      "neg  -  width_3\n",
      "neg  -  sq_ft_bin_501-1000\n",
      "neg  -  grade_Premium/A\n",
      "pos  -  month_3.0\n",
      "pos  -  sq_ft_bin_2001-5000\n"
     ]
    }
   ],
   "source": [
    "for feature in feature_order:\n",
    "    if lr.coef_[0][feature] > 0:\n",
    "        weight = 'pos'\n",
    "    else:\n",
    "        weight = 'neg'\n",
    "    \n",
    "    print weight, ' - ', x_names[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([[ 0.12217009,  0.12510245,  0.12954251,  0.13122185,  0.13100243,\n",
       "          0.13086346,  0.13070382,  0.13070211,  0.13070122],\n",
       "        [ 0.16400321,  0.1731583 ,  0.17356398,  0.17428575,  0.17404293,\n",
       "          0.17419563,  0.17419531,  0.17422919,  0.17421939],\n",
       "        [ 0.16245601,  0.165136  ,  0.16300537,  0.15985777,  0.15432829,\n",
       "          0.15419988,  0.1542084 ,  0.15420298,  0.15420462],\n",
       "        [ 0.14691294,  0.14375442,  0.1316308 ,  0.12735778,  0.12696433,\n",
       "          0.12694485,  0.12694617,  0.1269484 ,  0.1269484 ],\n",
       "        [ 0.186845  ,  0.18752846,  0.17335337,  0.16588318,  0.16412377,\n",
       "          0.16400972,  0.16396604,  0.16396545,  0.16395722],\n",
       "        [ 0.12697488,  0.13692581,  0.14342135,  0.14550258,  0.1459505 ,\n",
       "          0.14623612,  0.1462338 ,  0.14623589,  0.14623356],\n",
       "        [ 0.13652361,  0.14001409,  0.13779681,  0.13426573,  0.13308897,\n",
       "          0.13282992,  0.13250847,  0.1325014 ,  0.1325027 ],\n",
       "        [ 0.12355228,  0.13421198,  0.13889387,  0.14002117,  0.13981246,\n",
       "          0.13826295,  0.13679713,  0.136346  ,  0.13637537],\n",
       "        [ 0.13373449,  0.14604447,  0.1437273 ,  0.14579875,  0.14654394,\n",
       "          0.14652733,  0.14658855,  0.14659445,  0.14660363],\n",
       "        [ 0.14209399,  0.14900098,  0.14920959,  0.14776705,  0.14836061,\n",
       "          0.14861976,  0.14862077,  0.14863662,  0.14863662]])}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
